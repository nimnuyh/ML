{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Preprocessing & Feature Engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Modeling\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "from itertools import combinations\n",
    "from scipy.stats.mstats import gmean\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "np.random.seed(123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('tr_train.csv', encoding='utf-8')\n",
    "test = pd.read_csv('tr_test.csv', encoding='utf-8')\n",
    "cust_train = pd.read_csv('cust_train.csv', encoding='utf-8')\n",
    "cust_test = pd.read_csv('cust_test.csv', encoding = 'utf-8')\n",
    "\n",
    "train['PD_BUY_AM'] = train['PD_BUY_AM'].map(lambda x : int(str(x).replace(',','')))\n",
    "train['PD_BUY_CT']  = train['PD_BUY_CT'].map(lambda x: int(str(x).replace(',','')))\n",
    "test['PD_BUY_AM']  = test['PD_BUY_AM'].map(lambda x: int(str(x).replace(',','')))\n",
    "test['PD_BUY_CT']  = test['PD_BUY_CT'].map(lambda x: int(str(x).replace(',','')))\n",
    "\n",
    "train['TOT_PAG_VIEW_CT'].fillna(0, inplace=True)\n",
    "train['TOT_PAG_VIEW_CT'] = train['TOT_PAG_VIEW_CT'].apply(lambda x: int(x))\n",
    "\n",
    "test['TOT_PAG_VIEW_CT'].fillna(0, inplace=True)\n",
    "test['TOT_PAG_VIEW_CT'] = test['TOT_PAG_VIEW_CT'].apply(lambda x: int(x))\n",
    "\n",
    "train['TOT_SESS_HR_V'].fillna(0, inplace=True)\n",
    "train['TOT_SESS_HR_V'] = train['TOT_SESS_HR_V'].apply(lambda x: int(x.replace(',','')))\n",
    "\n",
    "test['TOT_SESS_HR_V'].fillna(0, inplace=True)\n",
    "test['TOT_SESS_HR_V'] = test['TOT_SESS_HR_V'].apply(lambda x: int(x.replace(',','')))\n",
    "\n",
    "train['date'] = pd.to_datetime(train['SESS_DT'], format='%Y%m%d')\n",
    "test['date'] = pd.to_datetime(test['SESS_DT'], format='%Y%m%d')\n",
    "\n",
    "features_train = []\n",
    "features_test = []\n",
    "train_add = []\n",
    "test_add = []\n",
    "indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(train, test, name):\n",
    "    features_train.append(train)\n",
    "    features_test.append(test)\n",
    "    train_add.append(True)\n",
    "    test_add.append(True)\n",
    "    indices.append(name)\n",
    "\n",
    "def oversample(x, n):\n",
    "    lst = []\n",
    "    for i in x:\n",
    "        tmp = []\n",
    "        for j in range(n):\n",
    "            random.shuffle(i)\n",
    "            tmp += list(i)\n",
    "        lst.append(tmp)\n",
    "    return lst\n",
    "\n",
    "def feature_vector(data, w2v):\n",
    "    mean_vector = []\n",
    "    for words in tqdm(data):\n",
    "        tmp = np.zeros(30)\n",
    "        cnt = 0\n",
    "        for word in words:\n",
    "            try:\n",
    "                tmp += w2v[word]\n",
    "                cnt += 1\n",
    "            except:\n",
    "                pass\n",
    "            tmp /= cnt\n",
    "        mean_vector.append(tmp)\n",
    "    return np.array(mean_vector)\n",
    "\n",
    "def w2v_features(train_data, test_data):\n",
    "    w2v_input = oversample(train_data, 5)\n",
    "    w2v = word2vec.Word2Vec(sentences = w2v_input, size = 30, window = 3, min_count = 1, sg = 1)\n",
    "    return feature_vector(train_data, w2v), feature_vector(test_data, w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_columns = ['CLAC3_NM','CLAC2_NM','PD_BRA_NM','KWD_NM']\n",
    "\n",
    "for col in w2v_columns:\n",
    "    train_data = list(train.groupby('CLNT_ID')[col].unique())\n",
    "    test_data = list(test.groupby('CLNT_ID')[col].unique())\n",
    "\n",
    "    train_mean_vector, test_mean_vector = w2v_features(train_data, test_data)\n",
    "\n",
    "    ftr = pd.concat([pd.DataFrame(cust_train.CLNT_ID),pd.DataFrame(train_mean_vector)], axis=1)\n",
    "    fte = pd.concat([pd.DataFrame(cust_test.CLNT_ID),pd.DataFrame(test_mean_vector)], axis=1)\n",
    "    add_feature(ftr, fte, 'w2v_'+col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(data=indices, columns=['feature'])\n",
    "final['train'] = train_add\n",
    "final['test'] = test_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final.iloc[:,1:] = False\n",
    "final.iloc[[29,31,32],1:] = True\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = [fe for idx, fe in enumerate(features_train) if final.train[idx]==True]\n",
    "final_test = [fe for idx, fe in enumerate(features_test) if final.test[idx]==True]\n",
    "\n",
    "data_train = pd.DataFrame({'CLNT_ID':np.sort(train.CLNT_ID.unique())})\n",
    "for f in final_train:\n",
    "    data_train = pd.merge(data_train, f, on=['CLNT_ID'], how='left')\n",
    "    \n",
    "data_test = pd.DataFrame({'CLNT_ID':np.sort(test.CLNT_ID.unique())})\n",
    "for f in final_test:\n",
    "    data_test = pd.merge(data_test, f, on=['CLNT_ID'], how='left')\n",
    "    \n",
    "data_train.fillna(0, inplace=True)\n",
    "data_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=4)\n",
    "n_it = 10\n",
    "params = {'n_estimators':[100], 'objective' : ['multiclass'], 'learning_rate' : [0.1], 'max_depth' : [10]}\n",
    "model = RandomizedSearchCV(LGBMClassifier(), param_distributions=params, n_iter=n_it, cv=kfold, verbose=1, n_jobs=-1, scoring='neg_log_loss')\n",
    "model.fit(data_train, cust_train.LABEL)\n",
    "print('========BEST_SCORE = ', model.best_score_)\n",
    "model_LGBM = model.best_estimator_\n",
    "\n",
    "pred_LGBMs = pd.DataFrame(model_LGBM.predict_proba(data_test))\n",
    "preds.append(pred_LGBMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=4)\n",
    "n_it = 10\n",
    "params = {'max_features':list(np.arange(1, train.shape[1])), 'min_samples_split' : [3, 5, 7], 'min_samples_leaf' : [1, 3, 5], 'max_depth' : [4,6,8], 'bootstrap':[True,False], 'n_estimators': [300, 500], 'criterion':['gini','entropy']}\n",
    "model = RandomizedSearchCV(RandomForestClassifier(), param_distributions=params, n_iter=n_it, cv=kfold, verbose=1, n_jobs=-1, scoring='neg_log_loss')\n",
    "model.fit(data_train, cust_train.LABEL)\n",
    "print('========BEST_SCORE = ', model.best_score_)\n",
    "model_RF = model.best_estimator_\n",
    "\n",
    "pred_RF = pd.DataFrame(model_RF.predict_proba(data_test))\n",
    "preds.append(pred_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=4)\n",
    "n_it = 10\n",
    "params = {'n_estimators': [300], 'max_depth':[7], 'objective':['multi:softmax'], 'learning_rate':[0.1]}\n",
    "model = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=n_it, cv=kfold, verbose=1, n_jobs=-1, scoring='neg_log_loss')\n",
    "model.fit(data_train, cust_train.LABEL)\n",
    "print('========BEST_SCORE = ', model.best_score_)\n",
    "model_XGB = model.best_estimator_\n",
    "\n",
    "pred_XGB = pd.DataFrame(model_XGB.predict_proba(data_test))\n",
    "preds.append(pred_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=4)\n",
    "n_it = 10\n",
    "params = {'penalty':['l1','l2','elasticnet'], 'C':[1, 0.5, 0.1], 'max_iter':[100,300,500]}\n",
    "model = RandomizedSearchCV(LogisticRegression(), param_distributions=params, n_iter=n_it, cv=kfold, verbose=1, n_jobs=-1, scoring='neg_log_loss')\n",
    "model.fit(data_train, cust_train.LABEL)\n",
    "print('========BEST_SCORE = ', model.best_score_)\n",
    "model_LR = model.best_estimator_\n",
    "\n",
    "pred_LR = pd.DataFrame(model_LR.predict_proba(data_test))\n",
    "preds.append(pred_LR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_pred = 0\n",
    "for p in preds:\n",
    "    last_pred += p\n",
    "last_pred /= len(preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([cust_test.CLNT_ID,pred_LGBMs],axis=1)\n",
    "result.columns = ['CLNT_ID','F20','F30','F40','M20','M30','M40']\n",
    "\n",
    "result.to_csv('submit/submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
